#!/usr/bin/python3

from utility import docker_util
from utility import mysql_util
from utility import logconfig

import json
import copy
import logging

standard_path = "data/standard/"
submission_path = "data/submission/"

CODE_FILES = "CODE_FILES"
OUTPUT_PROGRAM = "OUTPUT_PROGRAM"

#  used to cache the accumulated grade through each phases
#  reduce I/O in database
current_grade = 0

current_submission = {}

'''
Update grade after each judging phases

@Param grade
    grade to be added

@return
    no return value

'''
def update_grade(grade):
    global current_grade
    global current_submission

    current_grade += grade
    mysql_util.set_grade(current_submission["sub_id"], current_grade)


'''
Cut the string to 1024 bytes long, if possible

@Param string
    The string to be cut off

@return string
    Processed string
'''
def cut_string(string):
    if len(string) > 1024:
        return string[:1024]
    return string

'''
Get files's name that are needed during the life cycle

@Param config
    The configuration of a problem

@return
    A dict contains lists with standard_input, standard_output,
    submitted_files and standard files
'''
def get_needed_files(config):
    files = {}
    standard_files =  copy.deepcopy(config["standard"]["support"])
    for hidden_support in config["standard"]["hidden_support"]:
        standard_files.append(hidden_support)
    files["standard_files"] = standard_files
    files["standard_input"] = config["standard"]["standard_input"]
    files["standard_output"] = config["standard"]["standard_output"]
    files["submitted_files"] = config["submission"]
    files["random_files"] = config["standard"]["random_source"]
    return files

'''
Put files into sandbox

@Param prob_id
    problem id
@Param sub_id
    submission id
@file_name
    files name generated by get_needed_files(config)

@return
    no return value
'''
def put_files(file_name):
    standard_path_local = standard_path + str(current_submission["prob_id"]) + "/"
    submission_path_local = submission_path + str(current_submission["sub_id"]) + "/"
    files_path = []
    for input_file in file_name["standard_input"]:
        files_path.append(standard_path_local + "standard_input/" + input_file)

    for output_file in file_name["standard_output"]:
        files_path.append(standard_path_local + "standard_output/" + output_file)

    for submitted_file in file_name["submitted_files"]:
        files_path.append(submission_path_local + submitted_file)

    for support_files in file_name["standard_files"]:
        files_path.append(standard_path_local + "support/" + support_files)

    for support_files in file_name["random_files"]:
        files_path.append(standard_path_local + "random/" + support_files)

    docker_util.put_file(files_path, docker_util.sandbox_workspace)

'''
Delete all the files in the workspace
'''
def clear_workspace():
    logging.info("Clearing sandbox working space")
    files = docker_util.execute("ls")
    docker_util.execute("rm -rf " + files)

'''
Get all the source files, like *.cpp and *.c

@Param all_files
    file list that contains all the files
@Param ext
    filename extension to be found

@return
    A string contains the all matched files, seperated with blank spaces
'''
def find_src_file(all_files, ext):
    src_files = ""
    for f in all_files["standard_files"]:
        if ext in f:
            src_files += f + " "
    for f in all_files["submitted_files"]:
        if ext in f:
            src_files += f + " "
    return src_files

'''
Compiling phase

@Param sub_id
    submission id
@Param config
    problem configuration
@Param needed_files
    files that need to be compiled

@return compiling phase result
'''
def compile_submission(config, needed_files):
    logging.info(logconfig.log_formatter("Compiling",  submissionInfo))

    command = ""
    src_files = ""

    if config["standard_language"] == "c":
        command_raw = config["compilers"]["c"]["command"]
        src_files = find_src_file(needed_files, ".c")
    elif config["standard_language"] == "c++":
        command_raw = config["compilers"]["c++"]["command"]
        src_files = find_src_file(needed_files, ".cpp")

    command = command_raw.replace(
        CODE_FILES, src_files).replace(
            OUTPUT_PROGRAM, config["output_program"])

    compile_result = docker_util.execute(command)
    result = {}

    if compile_result == "":
        result = {
            "compile check": "pass",
            "continue": True,
            "grade": config["grading"]["compile check"]
        }
    else:
        result = {
            "compile check": compile_result,
            "continue": False,
            "grade": 0
        }
    update_grade(result["grade"])
    logging.debug(json.dumps(result))
    return result


'''
Static check phase

@Param submitted_files
    files to be checked in a list of string
@Param grade
    static check phase full grade

@return static check phase result
'''
def static_check(submitted_files, grade):
    logging.info(logconfig.log_formatter("Static checking",  submissionInfo))
    command = "oclint "
    for f in submitted_files:
        command += f + " "
    command += "--report-type=json -- -c -std=c++14"

    oclint_result_s = docker_util.execute(command)
    oclint_result = json.loads(oclint_result_s)

    for violation in oclint_result["summary"]["numberOfViolationsWithPriority"]:
        grade -= (3 - violation["priority"]) * violation["number"]

    result = {
        "grade": grade,
        "continue": True,
        "static check": oclint_result
    }
    update_grade(grade)
    logging.debug(json.dumps(result))
    return result

'''
Standard test phase

@Param config
    problem configuration

@return standard check phase result
'''
def standard_tests(config):
    logging.info(logconfig.log_formatter("Standard checking",  submissionInfo))

    language = config["standard_language"]
    limits = config["limits"]

    stdin_placeholder = "STDIN_FILE"

    result = {}
    result["standard tests"] = []
    correct_ans = 0
    wrong_ans = 0
    abnormal_ans = 0

    command = "/crun.py /policy/" + language + ".json " + \
              stdin_placeholder + " " + \
              str(limits["time"]) + " " + str(limits["memory"] * 1024) + \
              " ./" + config["output_program"]

    for it in range(0, len(config["standard"]["standard_input"])):
        crun_output_s = docker_util.execute(
            command.replace(
                stdin_placeholder, config["standard"]["standard_input"][it]))
        crun_output = json.loads(crun_output_s)
        # logging.debug(crun_output)

        standard_output = docker_util.execute("cat " + config["standard"]["standard_output"][it])

        if crun_output["result"] == "OK":
            if crun_output["stdout"] == standard_output:
                correct_ans += 1
            else:
                wrong_ans += 1
                if wrong_ans <= 1:
                    result["standard tests"].append(crun_output)
                    result["standard tests"][it]["result"] = "WA"
                    result["standard tests"][it]["standard_stdout"] = cut_string(standard_output)
                    result["standard tests"][it]["stdout"] = \
                        cut_string(result["standard tests"][it]["stdout"])
        else:
            abnormal_ans += 1
            if abnormal_ans <= 1:
                result["standard tests"].append(crun_output)
                standard_output = docker_util.execute("cat " + config["standard"]["standard_output"][it])
                result["standard tests"][it]["standard_stdout"] = cut_string(standard_output)
                result["standard tests"][it]["stdout"] = \
                        cut_string(result["standard tests"][it]["stdout"])

    result["grade"] = correct_ans / len(config["standard"]["standard_input"]) * \
                      config["grading"]["standard tests"]
    result["continue"] = True

    update_grade(result["grade"])
    logging.debug(json.dumps(result))
    return result


'''
Put standard files into sandbox and compile

@Param config
    problem configuration
@Param needed_files
    files needed to be compiled

@return
    no return value
'''
def compile_standard_files(config, needed_files):
    files_path = []
    for standard_file in needed_files["submitted_files"]:
        files_path.append(standard_path + str(current_submission["prob_id"]) + "/answer/" + standard_file)
    docker_util.put_file(files_path, docker_util.sandbox_workspace)

    command = ""
    src_files = ""

    if config["standard_language"] == "c":
        command_raw = config["compilers"]["c"]["command"]
        src_files = find_src_file(needed_files, ".c")
    elif config["standard_language"] == "c++":
        command_raw = config["compilers"]["c++"]["command"]
        src_files = find_src_file(needed_files, ".cpp")

    command = command_raw.replace(
        CODE_FILES, src_files).replace(
            OUTPUT_PROGRAM, config["entry_point"])

    compile_result = docker_util.execute(command)

    if compile_result != "":
        raise Exception("Failed to compile standard files\n" + compile_result)


'''
Random test phase

@Param config
    problem configuration
@Param needed_files
    files needed

@return
    random test result
'''
def random_tests(config, needed_files):
    logging.info(logconfig.log_formatter("Random checking",  submissionInfo))

    random_compile_command = config["random"]["compile_command"]
    random_file = ""
    for file in config["standard"]["random_source"]:
        random_file += file + " "
    compile_result = docker_util.execute(random_compile_command.replace("SOURCE", random_file))
    if compile_result != "":
        raise Exception("Failed to compile random source\n" + compile_result)

    compile_standard_files(config, needed_files)

    language = config["standard_language"]
    stdin_placeholder = "STDIN_FILE"
    executable_placeholder = "EXE"
    limits = config["limits"]
    command_raw = "/crun.py /policy/" + language + ".json " + \
              stdin_placeholder + " " + \
              str(limits["time"]) + " " + str(limits["memory"] * 1024) + \
              " ./" + executable_placeholder

    continue_flag = True
    correct_ans = 0
    wrong_ans = 0
    result = {}
    result["random tests"] = []
    for test in range(0, config["random"]["run_times"]):
        logging.info("Random case: " + str(test))
        #  cannot stream to a file...
        random_input = docker_util.execute("./random")
        logging.debug("input\n" + random_input)
        docker_util.put_strings([random_input],
                                ["random_input" + str(test)],
                                docker_util.sandbox_workspace)
        command = command_raw.replace(
            stdin_placeholder, "random_input" + str(test)).replace(
                executable_placeholder, config["entry_point"])
        random_output_crun_s = docker_util.execute(command)
        random_output_crun = json.loads(random_output_crun_s)

        if random_output_crun["result"] != "OK":
            raise Exception("Failed to get ouput from standard answer\n" + random_output_crun_s)

        random_output_standard = random_output_crun["stdout"]
        logging.debug("standard output\n" + random_output_standard)

        command = command_raw.replace(
            stdin_placeholder, "random_input" + str(test)).replace(
                executable_placeholder, config["output_program"])

        crun_output_s = docker_util.execute(command)
        crun_output = json.loads(crun_output_s)

        if crun_output["result"] == "OK":
            if crun_output["stdout"] == random_output_standard:
                correct_ans += 1
                if correct_ans == 1:
                    result["random tests"].append(crun_output)
                    result["random tests"][-1]["result"] = "CR"
                    result["random tests"][-1]["stdout"] = \
                        cut_string(result["random tests"][-1]["stdout"])
            else:
                wrong_ans += 1
                if wrong_ans <= 1:
                    result["random tests"].append(crun_output)
                    result["random tests"][-1]["result"] = "WA"
                    result["random tests"][-1]["standard_stdout"] = cut_string(random_output_standard)
                    result["random tests"][-1]["stdout"] = \
                        cut_string(result["random tests"][-1]["stdout"])
        else:
            abnormal_ans += 1
            if abnormal_ans <= 5:
                result["random tests"].append(crun_output)
                result["random tests"][-1]["standard_stdout"] = cut_string(random_output_standard)
                result["random tests"][-1]["stdout"] = \
                        cut_string(result["random tests"][-1]["stdout"])
            else:
                continue_flag = False
                break
    result["grade"] = correct_ans / config["random"]["run_times"] * \
                      config["grading"]["random tests"]
    result["continue"] = continue_flag

    update_grade(result["grade"])
    logging.debug(json.dumps(result))
    return result


def memory_check(config):
    logging.info(logconfig.log_formatter("Memory checking",  submissionInfo))

    stdin_placeholder = "STDIN_FILE"

    command_raw = "/memory.py ./" + config["output_program"] + " " + stdin_placeholder

    pass_times = 0
    test_times = 0

    result = {}
    result["memory check"] = []

    if config["grading"]["standard tests"] != 0:
        for stdin_file in config["standard"]["standard_input"]:
            logging.info("Standard case: " + stdin_file[-1])
            test_times += 1
            command = command_raw.replace(stdin_placeholder, stdin_file)
            valgrind_report_s = docker_util.execute(command)
            valgrind_report = json.loads(valgrind_report_s)

            logging.debug(valgrind_report_s)

            if "error" in valgrind_report:
                raise Exception(valgrind_report["error"])

            if "error" in valgrind_report["valgrindoutput"]:
                if isinstance(valgrind_report["valgrindoutput"]["error"], list):
                    valgrind_report["valgrindoutput"]["error"] = [valgrind_report["valgrindoutput"]["error"]]
                result["memory check"].append(valgrind_report)
            else:
                pass_times += 1

    elif config["grading"]["random tests"] != 0:
        for test in range(0, 10):
            logging.info("Standard case: " + str(test))
            test_times += 1
            command = command_raw.replace(stdin_placeholder, "random_input" + str(test))
            valgrind_report_s = docker_util.execute(command)
            valgrind_report = json.loads(valgrind_report_s)

            logging.debug(valgrind_report_s)

            if "error" in valgrind_report["valgrindoutput"]:
                if isinstance(valgrind_report["valgrindoutput"]["error"], list):
                    valgrind_report["valgrindoutput"]["error"] = [valgrind_report["valgrindoutput"]["error"]]
                result["memory check"].append(valgrind_report)
            else:
                pass_times += 1

    result["grade"] = pass_times / test_times * \
                      config["grading"]["memory check"]
    result["continue"] = False

    update_grade(result["grade"])
    logging.debug(json.dumps(result))
    return result


if __name__ == "__main__":
    docker_util.start_container()
    waiting_queue = mysql_util.get_waiting_id()

    for submission in waiting_queue:
        submissionInfo = json.dumps(submission)
        logging.info("Judging: " + submissionInfo)
        current_submission = submission
        current_grade = 0
        phase = ""

        report = {
            "submission_id": str(current_submission["sub_id"])
        }

        try:

        #  load problem configuration
            try:
                prob_config = json.loads(mysql_util.get_problem_config(submission["prob_id"]))
            except Exception as e:
                logging.error("Getting problem configuration failed. " + submissionInfo)
                logging.exception(e)

            #  get needed files and put them into sandbox
            needed_files = get_needed_files(prob_config)
            put_files(needed_files)

            #  compile
            try:
                report["compile check"] = compile_submission(prob_config, needed_files)
                phase = "compile check"
            except Exception as e:
                logging.error(logconfig.log_formatter("Compiling phase failed ",  submissionInfo))
                report["error"] = "Internal error"
                raise e

            #  static check
            if report[phase]["continue"] and prob_config["grading"]["static check"] != 0:
                try:
                    report["static check"] = static_check(
                        prob_config["submission"], prob_config["grading"]["static check"])
                    phase = "static check"
                except Exception as e:
                    logging.error(logconfig.log_formatter("Static check phase failed ",  submissionInfo))
                    report["error"] = "Internal error"
                    raise e

            #  standard test
            if report[phase]["continue"] and prob_config["grading"]["standard tests"] != 0:
                try:
                    report["standard tests"] = standard_tests(prob_config)
                    phase = "standard tests"
                except Exception as e:
                    logging.error(logconfig.log_formatter("Standard check phase failed ",  submissionInfo))
                    report["error"] = "Internal error"
                    raise e

            #  random test
            if report[phase]["continue"] and prob_config["grading"]["random tests"] != 0:
                try:
                    report["random tests"] = random_tests(prob_config, needed_files)
                    phase = "random tests"
                except Exception as e:
                    logging.error(logconfig.log_formatter("Random test phase failed ",  submissionInfo))
                    report["error"] = "Internal error"
                    raise e

            #  memory check
            if report[phase]["continue"] and prob_config["grading"]["memory check"] != 0:
                try:
                    report["memory check"] = memory_check(prob_config)
                    phase = "memory check"
                except Exception as e:
                    logging.error(logconfig.log_formatter("Memory check phase failed ",  submissionInfo))
                    report["error"] = "Internal error"
                    raise e

            report["total_grade"] = current_grade
        except Exception as e:
            logging.exception(e)
            logging.error(logconfig.log_formatter("Judging aborted ",  submissionInfo))

        report_s = json.dumps(report)
        try:
            mysql_util.set_submission_report(current_submission["sub_id"], report_s)
        except Exception as e:
            logging.exception(e)
            logging.error(logconfig.log_formatter("Submitting submission report failed ",  submissionInfo))
            logging.warning("Report is:\n", report_s)

        clear_workspace()
        logging.info(logconfig.log_formatter("Judging complete",  submissionInfo + "\n"))
